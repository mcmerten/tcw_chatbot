{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.001272Z",
     "start_time": "2023-06-24T14:44:20.889908Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/merten/Development/tcw_chatbot/venv/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil\n",
    "import requests\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.004590Z",
     "start_time": "2023-06-24T14:44:24.002124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load secrets to access API\n",
    "load_dotenv()\n",
    "#os.environ['OPENAI_API_KEY'] = os.environ.get('OPENAI_API_TOKEN')\n",
    "#openai.api_key = os.environ.get('OPENAI_API_TOKEN')\n",
    "\n",
    "# constants\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.006695Z",
     "start_time": "2023-06-24T14:44:24.005128Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# Define root domain to crawl\n",
    "domain = \"tcw.de\"\n",
    "full_url = \"https://tcw.de/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.009169Z",
     "start_time": "2023-06-24T14:44:24.007654Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.012135Z",
     "start_time": "2023-06-24T14:44:24.010780Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get_content_type() == \"text/html\":\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.015127Z",
     "start_time": "2023-06-24T14:44:24.013455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif (\n",
    "                link.startswith(\"#\")\n",
    "                or link.startswith(\"mailto:\")\n",
    "                or link.startswith(\"tel:\")\n",
    "            ):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.017414Z",
     "start_time": "2023-06-24T14:44:24.015914Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing and to avoid crawling all pages\n",
    "def is_blacklisted(url):\n",
    "    blacklist = [\"https://tcw.de/uploads\",\n",
    "                 \"https://tcw.de/fachliteratur\",\n",
    "                 \"https://tcw.de/publikationen\",\n",
    "                 \"https://tcw.de/impressum\",\n",
    "                 \"https://tcw.de/news\",\n",
    "                ]\n",
    "    #blacklist = []\n",
    "    for blacklisted_url in blacklist:\n",
    "        if blacklisted_url in url:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.036323Z",
     "start_time": "2023-06-24T14:44:24.019232Z"
    }
   },
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = {url}\n",
    "\n",
    "    # Create a directory to store the raw html files\n",
    "    if not os.path.exists(\"scraper/data/\"):\n",
    "        os.makedirs(\"scraper/data/\")\n",
    "    \n",
    "    if not os.path.exists(\"scraper/data/\"+local_domain+\"/\"):\n",
    "           os.makedirs(\"scraper/data/\" + local_domain + \"/\")\n",
    "            \n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "    \n",
    "    # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(f\"{url} ({len(queue)})\") # for debugging and to see the progress\n",
    "        \n",
    "        # Define destination\n",
    "        file_name = local_domain+'/'+url[8:].replace(\"/\", \"_\") \n",
    "        \n",
    "        resp = requests.get(url)\n",
    "        # Request content and save in distinct file\n",
    "        if resp.headers.get('Content-Type').startswith('text/html'):\n",
    "            html_content = resp.text\n",
    "            try:\n",
    "                with open('scraper/data/' + file_name + '.html', 'w') as f:\n",
    "                    f.write(html_content)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen and not is_blacklisted(link):\n",
    "                queue.append(link)\n",
    "                seen.add(link) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.215742Z",
     "start_time": "2023-06-24T14:44:24.209855Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions that adds a new metadata field containing the source url of each HTML file\n",
    "def add_source_url(elements):\n",
    "    for element in elements:\n",
    "        source_url = \"https://\" + element.metadata[\"source\"].split(\"/\")[1]\\\n",
    "                                                            .replace(\"_\", \"/\")\\\n",
    "                                                            .removesuffix(\".html\")\n",
    "        element.metadata[\"source\"] = source_url\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.447547Z",
     "start_time": "2023-06-24T14:44:24.439273Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a function that takes a element and modifies the page_content by removing HTML tags\n",
    "def remove_html_tags(elements):\n",
    "    for element in elements:\n",
    "        element.page_content = re.sub('<[^<]+?>', ' ', element.page_content)\n",
    "        element.page_content = re.sub(r'<!--.*?-->', '', element.page_content)\n",
    "        # remove beginning of HTML comments\n",
    "        element.page_content = re.sub(r'<!--.*', '', element.page_content)\n",
    "        # remove end of HTML comments\n",
    "        element.page_content = re.sub(r'.*-*>', '', element.page_content)\n",
    "        element.page_content = element.page_content.strip()\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.739102Z",
     "start_time": "2023-06-24T14:44:24.734907Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(elements):\n",
    "    seen = set()\n",
    "    new_elements = []\n",
    "    for element in elements:\n",
    "        if element.page_content not in seen:\n",
    "            seen.add(element.page_content)\n",
    "            new_elements.append(element)\n",
    "    return new_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:24.991519Z",
     "start_time": "2023-06-24T14:44:24.987158Z"
    }
   },
   "outputs": [],
   "source": [
    "# open temporarily stored HTML files and read relevant content from respective class with BS4\n",
    "# Use unstructured to go over the retrieved section and structure the data by elements (e.g. title, text, list, etc.)\n",
    "# store unstructured objects in a list\n",
    "\n",
    "def retrieve_relevant_content():\n",
    "    seen = set()\n",
    "    relevant_content = []\n",
    "    # create a tmp folder to store the text files which is deleted after the function is executed\n",
    "    if not os.path.exists(\"tmp/\"):\n",
    "        os.makedirs(\"tmp/\")\n",
    "    for file in os.listdir(\"scraper/data/\" + domain + \"/\"):\n",
    "        with open(\"scraper/data/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "                text = str(soup.find(\"div\", class_=\"content_frame_out\"))\n",
    "                # Create a temporary file to store the text\n",
    "                with open(\"tmp/\" + file, \"w\", encoding=\"UTF-8\") as f:\n",
    "                    f.write(text)\n",
    "    # iterate over file in tmp folder and create UnstructuredFileLoader object and read the files from the tmp folder  \n",
    "    for file in os.listdir(\"tmp/\"):\n",
    "        loader = UnstructuredFileLoader(\"tmp/\" + file, strategy=\"hi_res\", mode=\"elements\")\n",
    "        document = loader.load()\n",
    "        document = add_source_url(document)\n",
    "        # not all tags are removed by the unstructured library, so we need to remove them manually\n",
    "        document = remove_html_tags(document)\n",
    "        #document = remove_duplicates(document)\n",
    "        # within the file_data object, iterate over the documents and apppend only the elements with metadata.category == \"NarrativeText\" and sentence_count(element.page_content) > 1\n",
    "        \n",
    "        for element in document:\n",
    "            if element.page_content not in seen and (element.metadata[\"category\"] == \"NarrativeText\" or element.metadata[\"category\"] == \"ListItem\"):\n",
    "                seen.add(element.page_content)\n",
    "                relevant_content.append(element)\n",
    "        \n",
    "        #relevant_content.append(document)\n",
    "        #for doc in relevant_content:\n",
    "        #    for element in doc:\n",
    "        #        if not (element.metadata[\"category\"] == \"NarrativeText\" or element.metadata[\"category\"] == \"ListItem\"):\n",
    "        #            doc.remove(element)\n",
    "        #filtered_relevant_content = list(filter(None, relevant_content))\n",
    "    # delete the tmp folder\n",
    "    shutil.rmtree(\"tmp/\")\n",
    "    return relevant_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T14:44:29.434548Z",
     "start_time": "2023-06-24T14:44:25.223278Z"
    }
   },
   "outputs": [],
   "source": [
    "final_content = retrieve_relevant_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# flatten each document element into  an object with the following structure: {page_content, source_url, filetype, category}\n",
    "def flatten_document(document):\n",
    "    flattened_document = []\n",
    "    for element in document:\n",
    "        flattened_document.append({\"page_content\": element.page_content, \"source_url\": element.metadata[\"source\"], \"filetype\": element.metadata[\"filetype\"], \"category\": element.metadata[\"category\"]})\n",
    "    return flattened_document\n",
    "\n",
    "data = flatten_document(final_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T19:47:47.840812Z",
     "start_time": "2023-06-24T19:47:47.835316Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T19:36:47.270421Z",
     "start_time": "2023-06-24T19:36:46.461463Z"
    }
   },
   "outputs": [],
   "source": [
    "pinecone.init(environment=\"us-west1-gcp-free\", api_key=os.environ.get(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {},\n 'total_vector_count': 0}"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(\"tcw-website-embeddings\")\n",
    "index.describe_index_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T19:36:49.181778Z",
     "start_time": "2023-06-24T19:36:48.563156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T15:14:47.114085Z",
     "start_time": "2023-06-24T15:14:47.102998Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T19:51:53.886379Z",
     "start_time": "2023-06-24T19:51:53.880982Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vector_db(content_documents):\n",
    "    emeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Define TextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(content_documents)\n",
    "    index_name=\"tcw-website-embeddings\"\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        pinecone.create_index(\n",
    "            name=index_name,\n",
    "            metric='cosine',\n",
    "            dimension=1536 # 1536 dim of text-embedding-ada-002\n",
    "        )\n",
    "    index = pinecone.Index(index_name)\n",
    "    index.describe_index_stats()\n",
    "\n",
    "\n",
    "    batch_limit = 100\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for i, record in enumerate(tqdm(data)):\n",
    "        # first get metadata fields for this record\n",
    "        metadata = {\n",
    "            'source_url': str(record['source_url']),\n",
    "            'filetype': record['filetype'],\n",
    "            'category': record['category']\n",
    "        }\n",
    "        # now we create chunks from the record text\n",
    "        record_texts = text_splitter.split_text(record['page_content'])\n",
    "        # create individual metadata dicts for each chunk\n",
    "        record_metadatas = [{\n",
    "            \"chunk\": j, \"text\": text, **metadata\n",
    "        } for j, text in enumerate(record_texts)]\n",
    "        # append these to current batches\n",
    "        texts.extend(record_texts)\n",
    "        metadatas.extend(record_metadatas)\n",
    "        # if we have reached the batch_limit we can add texts\n",
    "        if len(texts) >= batch_limit:\n",
    "            ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "            embeds = emeddings.embed_documents(texts)\n",
    "            index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "            texts = []\n",
    "            metadatas = []\n",
    "\n",
    "    if len(texts) > 0:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = emeddings.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "    index.describe_index_stats()\n",
    "\n",
    "    print(\"Database created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/653 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1f7c706344d4815940f64718cb1de97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created\n"
     ]
    }
   ],
   "source": [
    "create_vector_db(final_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T19:56:46.802058Z",
     "start_time": "2023-06-24T19:51:54.907355Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(\"tcw-website-embeddings\")\n",
    "emeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, emeddings.embed_query, text_field\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T20:05:12.467345Z",
     "start_time": "2023-06-24T20:05:12.460669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Prof. Wildemann', metadata={'category': 'ListItem', 'chunk': 0.0, 'filetype': 'text/html', 'source_url': 'https://tcw.de/sitemap'}),\n Document(page_content='In 40 Büchern und über 700 Aufsätzen, die in engem Kontakt mit der Praxis entstanden sind, hat er neue Wege für die wirtschaftliche Gestaltung von Unternehmen mit Zukunft aufgezeigt. Durch die Kombination von Forschungsinstitut und Management Consulting schafft er es immer wieder, die gewonnen Ergebnisse aus Forschung und unternehmerischer Praxis zu integrieren. Für führende Industrieunternehmen ist Professor Wildemann als Berater, Aufsichts- und Beiratsmitglied tätig. Ihm wurden die Staatsmedaille des Freistaates Bayern, das Bundesverdienstkreuz 1. Klasse der Bundesrepublik Deutschland und die Ehrendoktorwürden der Universitäten Klagenfurt, Passau und Cottbus verliehen. Seit 2004 ist er in die Logistik Hall of Fame aufgenommen worden. 2006 erhielt Professor Wildemann vom Bayerischen Ministerpräsidenten den Bayerischen Verdienstorden für seine herausragenden Leistungen in Wissenschaft und Industrie. Im Jahr 2008 wurde ihm die Ehrennadel der Bundesvereinigung Logistik verliehen.', metadata={'category': 'NarrativeText', 'chunk': 0.0, 'filetype': 'text/html', 'source_url': 'https://tcw.de/unternehmen/sonstiges/prof-wildemann-4'}),\n Document(page_content='Univ.-Prof. Dr. Dr. h. c. mult. Horst Wildemann studierte in Aachen und Köln Maschinenbau (Dipl.-Ing.) und Betriebswirtschaftslehre (Dipl.-Kfm.). Nach einer mehrjährigen praktischen Tätigkeit als Ingenieur in der Automobilindustrie promovierte er 1974 zum Dr. rer. pol., Auslandsaufenthalte am Internationalen Management Institut in Brüssel und an amerikanischen Universitäten schlossen sich an. 1980 habilitierte er sich  an der Universität zu Köln (Dr. habil.). Seit 1980 lehrt Wildemann als ordentlicher Professor für Betriebswirtschaftslehre an den Universitäten Bayreuth, Passau und seit 1989 an der Technischen Universität München. Er hat Rufe an die Universitäten Stuttgart Hohenheim und Dortmund, an die Freie und die Technische Universität Berlin, University of Southern California, Los Angeles, University of Indianapolis, Indianapolis, und an die Hochschule St. Gallen erhalten. Neben seiner Lehrtätigkeit steht Prof. Wildemann der Unternehmensberatung TCW Transfer-Centrum GmbH & Co. KG', metadata={'category': 'NarrativeText', 'chunk': 0.0, 'filetype': 'text/html', 'source_url': 'https://tcw.de/unternehmen/sonstiges/prof-wildemann-4'})]"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Wer ist Prof. Dr. Wildemann?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-24T20:05:43.193861Z",
     "start_time": "2023-06-24T20:05:42.022086Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename function to main and run as one piece\n",
    "def webscraper():\n",
    "     #crawl(full_url)\n",
    "     #print(\"Crawling successful!\")\n",
    "     relevant_content = retrieve_relevant_content()\n",
    "     print(\"Relevant content retrieved!\")\n",
    "     create_vector_db(relevant_content)\n",
    "     print(\"ChromaDB successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_db(relevant_content) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma(persist_directory=\"chroma_db_single_mode\", embedding_function=embeddings, collection_name=\"tcw_chroma_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search_with_score(\"Wer ist Prof. Dr. Wildemann?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search_with_score(\"Was ist die 5-S-Methode?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.max_marginal_relevance_search(\"Wer ist Prof. Dr. Wildemann?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet('chroma_db/chroma-embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "%sql duckdb:///:memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "describe table 'chroma_db/chroma-embeddings.parquet';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "create table embeddings as\n",
    "select * \n",
    "from 'chroma_db/chroma-embeddings.parquet';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select document from embeddings where metadata LIKE '%5-s-konzept-als%' LIMIT 100;\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client(Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",\n",
    "    persist_directory=\"chroma_db\",   \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"tcw_chroma_collection\", embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    where={\"source_url\": \"https://tcw.de/news/komplexitaet-in-der-beschaffung-abbauen-180\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"source\": \"tmp/tcw.de_news_komplexitaet-in-der-beschaffung-abbauen-180.html\", \"filename\": \"tmp/tcw.de_news_komplexitaet-in-der-beschaffung-abbauen-180.html\", \"category\": \"NarrativeText\", \"source_url\": \"https://tcw.de/news/komplexitaet-in-der-beschaffung-abbauen-180\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
