{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "#from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load secrets to access API\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get('OPENAI_API_TOKEN')\n",
    "#openai.api_key = os.environ.get('OPENAI_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# Define root domain to crawl\n",
    "domain = \"tcw.de\"\n",
    "full_url = \"https://tcw.de/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get_content_type() == \"text/html\":\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif (\n",
    "                link.startswith(\"#\")\n",
    "                or link.startswith(\"mailto:\")\n",
    "                or link.startswith(\"tel:\")\n",
    "            ):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing and to avoid crawling all pages\n",
    "def is_blacklisted(url):\n",
    "    blacklist = [\"https://tcw.de/uploads\",\n",
    "                 \"https://tcw.de/fachliteratur\",\n",
    "                 \"https://tcw.de/publikationen\",\n",
    "                 \"https://tcw.de/impressum\",\n",
    "                 \"https://tcw.de/news\",\n",
    "                ]\n",
    "    #blacklist = []\n",
    "    for blacklisted_url in blacklist:\n",
    "        if blacklisted_url in url:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the raw html files\n",
    "    if not os.path.exists(\"scraper/data/\"):\n",
    "        os.makedirs(\"scraper/data/\")\n",
    "    \n",
    "    if not os.path.exists(\"scraper/data/\"+local_domain+\"/\"):\n",
    "           os.makedirs(\"scraper/data/\" + local_domain + \"/\")\n",
    "            \n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "    \n",
    "    # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(f\"{url} ({len(queue)})\") # for debugging and to see the progress\n",
    "        \n",
    "        # Define destination\n",
    "        file_name = local_domain+'/'+url[8:].replace(\"/\", \"_\") \n",
    "        \n",
    "        resp = requests.get(url)\n",
    "        # Request content and save in distinct file\n",
    "        if resp.headers.get('Content-Type').startswith('text/html'):\n",
    "            html_content = resp.text\n",
    "            try:\n",
    "                with open('scraper/data/' + file_name + '.html', 'w') as f:\n",
    "                    f.write(html_content)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen and not is_blacklisted(link):\n",
    "                queue.append(link)\n",
    "                seen.add(link) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that adds a new metadata field containing the source url of each HTML file\n",
    "def add_source_url(elements):\n",
    "    for element in elements:\n",
    "        source_url = \"https://\" + element.metadata[\"source\"].split(\"/\")[1]\\\n",
    "                                                            .replace(\"_\", \"/\")\\\n",
    "                                                            .removesuffix(\".html\")\n",
    "        element.metadata[\"source\"] = source_url\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a element and modifies the page_content by removing HTML tags\n",
    "def remove_html_tags(elements):\n",
    "    for element in elements:\n",
    "        element.page_content = re.sub('<[^<]+?>', ' ', element.page_content)\n",
    "        element.page_content = re.sub(r'<!--.*?-->', '', element.page_content)\n",
    "        # remove beginning of HTML comments\n",
    "        element.page_content = re.sub(r'<!--.*', '', element.page_content)\n",
    "        # remove end of HTML comments\n",
    "        element.page_content = re.sub(r'.*-*>', '', element.page_content)\n",
    "        element.page_content = element.page_content.strip()\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(elements):\n",
    "    seen = set()\n",
    "    new_elements = []\n",
    "    for element in elements:\n",
    "        if element.page_content not in seen:\n",
    "            seen.add(element.page_content)\n",
    "            new_elements.append(element)\n",
    "    return new_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open temporarily stored HTML files and read relevant content from respective class with BS4\n",
    "# Use unstructured to go over the retrieved section and structure the data by elements (e.g. title, text, list, etc.)\n",
    "# store unstructured objects in a list\n",
    "\n",
    "def retrieve_relevant_content():\n",
    "    seen = set()\n",
    "    relevant_content = []\n",
    "    # create a tmp folder to store the text files which is deleted after the function is executed\n",
    "    if not os.path.exists(\"tmp/\"):\n",
    "        os.makedirs(\"tmp/\")\n",
    "    for file in os.listdir(\"scraper/data/\" + domain + \"/\"):\n",
    "        with open(\"scraper/data/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "                text = str(soup.find(\"div\", class_=\"content_frame_out\"))\n",
    "                # Create a temporary file to store the text\n",
    "                with open(\"tmp/\" + file, \"w\", encoding=\"UTF-8\") as f:\n",
    "                    f.write(text)\n",
    "    # iterate over file in tmp folder and create UnstructuredFileLoader object and read the files from the tmp folder  \n",
    "    for file in os.listdir(\"tmp/\"):\n",
    "        loader = UnstructuredFileLoader(\"tmp/\" + file, strategy=\"hi_res\", mode=\"elements\")\n",
    "        document = loader.load()\n",
    "        document = add_source_url(document)\n",
    "        # not all tags are removed by the unstructured library, so we need to remove them manually\n",
    "        document = remove_html_tags(document)\n",
    "        #document = remove_duplicates(document)\n",
    "        # within the file_data object, iterate over the documents and apppend only the elements with metadata.category == \"NarrativeText\" and sentence_count(element.page_content) > 1\n",
    "        \n",
    "        for element in document:\n",
    "            if element.page_content not in seen and (element.metadata[\"category\"] == \"NarrativeText\" or element.metadata[\"category\"] == \"ListItem\"):\n",
    "                seen.add(element.page_content)\n",
    "                relevant_content.append(element)\n",
    "        \n",
    "        #relevant_content.append(document)\n",
    "        #for doc in relevant_content:\n",
    "        #    for element in doc:\n",
    "        #        if not (element.metadata[\"category\"] == \"NarrativeText\" or element.metadata[\"category\"] == \"ListItem\"):\n",
    "        #            doc.remove(element)\n",
    "        #filtered_relevant_content = list(filter(None, relevant_content))\n",
    "    # delete the tmp folder\n",
    "    shutil.rmtree(\"tmp/\")\n",
    "    return relevant_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_content = retrieve_relevant_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'estimator_export' from 'tensorflow.python.util.tf_export' (/Users/merten/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/tf_export.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorflowHubEmbeddings\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhub\u001b[39;00m\n\u001b[1;32m      4\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://tfhub.dev/google/universal-sentence-encoder/4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m tf \u001b[39m=\u001b[39m TensorflowHubEmbeddings(model_url\u001b[39m=\u001b[39murl)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow_hub/__init__.py:88\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mThis version of tensorflow_hub requires tensorflow \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mversion >= \u001b[39m\u001b[39m{required}\u001b[39;00m\u001b[39m; Detected an installation of version \u001b[39m\u001b[39m{present}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m             required\u001b[39m=\u001b[39mrequired_tensorflow_version,\n\u001b[1;32m     83\u001b[0m             present\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39m__version__))\n\u001b[1;32m     85\u001b[0m _ensure_tf_install()\n\u001b[0;32m---> 88\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m LatestModuleExporter\n\u001b[1;32m     89\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m register_module_for_export\n\u001b[1;32m     90\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m image_embedding_column\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow_hub/estimator.py:62\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mThere is already a module registered to be exported as \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[39m%\u001b[39m export_name)\n\u001b[1;32m     58\u001b[0m   tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39madd_to_collection(_EXPORT_MODULES_COLLECTION,\n\u001b[1;32m     59\u001b[0m                                  (export_name, module))\n\u001b[0;32m---> 62\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLatestModuleExporter\u001b[39;00m(tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mExporter):\n\u001b[1;32m     63\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Regularly exports registered modules into timestamped directories.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m  Warning: Deprecated. This belongs to the hub.Module API and TF1 Hub format.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m  THIS FUNCTION IS DEPRECATED.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, serving_input_fn, exports_to_keep\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:58\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[0;32m---> 58\u001b[0m   module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n\u001b[1;32m     59\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, item)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n\u001b[1;32m     44\u001b[0m \u001b[39m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"Estimator: High level tools for working with models.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m export\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/experimental/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"Public API for tf.estimator.experimental namespace.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdnn\u001b[39;00m \u001b[39mimport\u001b[39;00m dnn_logit_fn_builder\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkmeans\u001b[39;00m \u001b[39mimport\u001b[39;00m KMeansClustering \u001b[39mas\u001b[39;00m KMeans\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearSDCA\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_column_lib\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_export\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_estimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mestimator\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcanned\u001b[39;00m \u001b[39mimport\u001b[39;00m head \u001b[39mas\u001b[39;00m head_lib\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'estimator_export' from 'tensorflow.python.util.tf_export' (/Users/merten/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/tf_export.py)"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import TensorflowHubEmbeddings\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "tf = TensorflowHubEmbeddings(model_url=url)\n",
    "\n",
    "    # Define TextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(relevant_content)\n",
    "embeddings = tf.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vector_db(content_documents):\n",
    "\n",
    "\n",
    "    # Define TextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(content_documents)\n",
    "    #for doc in content_documents:\n",
    "    #    texts.append(text_splitter.split_documents(doc))\n",
    "    elements = [text for sublist in chunks for text in sublist]\n",
    "    # Define Embedding\n",
    "    Chroma.from_documents(documents = chunks,\n",
    "                          embedding= tf,\n",
    "                          collection_name=\"tcw_chroma_collection\",\n",
    "                          persist_directory=\"chroma_db_tf_embeddings\")\n",
    "    print(\"Database created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def webscraper():\n",
    "     #crawl(full_url)\n",
    "     #print(\"Crawling successful!\")\n",
    "     relevant_content = retrieve_relevant_content()\n",
    "     print(\"Relevant content retrieved!\")\n",
    "     create_vector_db(relevant_content)\n",
    "     print(\"ChromaDB successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_db(relevant_content) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma(persist_directory=\"chroma_db_single_mode\", embedding_function=embeddings, collection_name=\"tcw_chroma_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search_with_score(\"Wer ist Prof. Dr. Wildemann?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search_with_score(\"Was ist die 5-S-Methode?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.max_marginal_relevance_search(\"Wer ist Prof. Dr. Wildemann?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet('chroma_db/chroma-embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "%sql duckdb:///:memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "describe table 'chroma_db/chroma-embeddings.parquet';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "create table embeddings as\n",
    "select * \n",
    "from 'chroma_db/chroma-embeddings.parquet';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select document from embeddings where metadata LIKE '%5-s-konzept-als%' LIMIT 100;\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client(Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",\n",
    "    persist_directory=\"chroma_db\",   \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"tcw_chroma_collection\", embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    where={\"source_url\": \"https://tcw.de/news/komplexitaet-in-der-beschaffung-abbauen-180\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"source\": \"tmp/tcw.de_news_komplexitaet-in-der-beschaffung-abbauen-180.html\", \"filename\": \"tmp/tcw.de_news_komplexitaet-in-der-beschaffung-abbauen-180.html\", \"category\": \"NarrativeText\", \"source_url\": \"https://tcw.de/news/komplexitaet-in-der-beschaffung-abbauen-180\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
